name: æå–

on:
  schedule:
    - cron: '45 */6 * * *'
  workflow_dispatch:

jobs:
  extract:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout current repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Download XML files
        run: |
          curl -L --retry 3 --max-time 10 -o twepg.xml https://raw.githubusercontent.com/zzq1234567890/epg/main/twepg.xml
          curl -L --retry 3 --max-time 10 -o swepg.xml https://raw.githubusercontent.com/zzq1234567890/epg/main/swepg.xml
          [ -s twepg.xml ] || { echo "twepg.xml ä¸ºç©º"; exit 1; }
          [ -s swepg.xml ] || { echo "swepg.xml ä¸ºç©º"; exit 1; }

      - name: Extract channel names and generate JSON analysis
        run: |
          python <<'EOF'
          import xml.etree.ElementTree as ET
          import json
          from datetime import datetime
          import re

          def analyze_epg_data(xml_file):
              try:
                  tree = ET.parse(xml_file)
                  root = tree.getroot()
                  
                  ns = {}
                  if root.tag.startswith('{'):
                      ns_uri = root.tag.split('}')[0][1:]
                      ns = {'ns': ns_uri}
                  
                  channels_info = {}
                  channel_xpath = 'ns:channel' if ns else 'channel'
                  for channel in root.findall(channel_xpath, ns):
                      channel_id = channel.get('id')
                      if not channel_id:
                          continue
                      name = channel.get('display-name')
                      if not name:
                          name_elem = channel.find('ns:display-name' if ns else 'display-name', ns)
                          name = name_elem.text.strip() if name_elem is not None and name_elem.text else None
                      name = name.strip() if name else channel_id
                      
                      channels_info[channel_id] = {
                          'name': name,
                          'program_count': 0,
                          'start_times': [],
                          'end_times': []
                      }
                  
                  programme_xpath = 'ns:programme' if ns else 'programme'
                  for idx, programme in enumerate(root.findall(programme_xpath, ns)):
                      channel_id = programme.get('channel')
                      start_time = programme.get('start')
                      end_time = programme.get('stop')
                      
                      if channel_id not in channels_info:
                          continue
                      
                      channels_info[channel_id]['program_count'] += 1
                      
                      if start_time:
                          clean_start = re.sub(r'\s+[+-]\d{4}$', '', start_time)
                          channels_info[channel_id]['start_times'].append(clean_start)
                      if end_time:
                          clean_end = re.sub(r'\s+[+-]\d{4}$', '', end_time)
                          channels_info[channel_id]['end_times'].append(clean_end)
                  
                  result = []
                  for channel_id, info in channels_info.items():
                      time_range = "æš‚æ— èŠ‚ç›®æ•°æ®"
                      if info['start_times'] and info['end_times']:
                          try:
                              sorted_starts = sorted(info['start_times'])
                              sorted_ends = sorted(info['end_times'])
                              earliest_start = sorted_starts[0]
                              latest_end = sorted_ends[-1]
                              
                              start_dt = datetime.strptime(earliest_start, '%Y%m%d%H%M%S')
                              end_dt = datetime.strptime(latest_end, '%Y%m%d%H%M%S')
                              time_range = f"{start_dt.strftime('%Y-%m-%d %H:%M:%S')} è‡³ {end_dt.strftime('%Y-%m-%d %H:%M:%S')}"
                          except ValueError as e:
                              time_range = f"{earliest_start} è‡³ {latest_end}ï¼ˆæ ¼å¼å¼‚å¸¸ï¼‰"
                      
                      epg_status = "â– " if info['program_count'] > 0 else "â–¡"
                      
                      result.append({
                          'channel_id': channel_id,
                          'channel_name': info['name'],
                          'program_count': info['program_count'],
                          'time_range': time_range,
                          'epg_status': epg_status
                      })
                  
                  result.sort(key=lambda x: x['channel_name'].lower())
                  return result
                  
              except Exception as e:
                  print(f"âŒ è§£æ {xml_file} å‡ºé”™: {str(e)}")
                  return []

          print("å¼€å§‹åˆ†æ EPG æ•°æ®...")
          tw_data = analyze_epg_data('twepg.xml')
          sw_data = analyze_epg_data('swepg.xml')

          def generate_table(data, title):
              return {
                  'title': title,
                  'headers': ['é¢‘é“ID', 'é¢‘é“åç§°', 'èŠ‚ç›®æ•°é‡', 'æ—¶é—´èŒƒå›´', 'EPGçŠ¶æ€'],
                  'data': [[
                      ch['channel_id'],
                      ch['channel_name'],
                      ch['program_count'],
                      ch['time_range'],
                      ch['epg_status']
                  ] for ch in data]
              }

          tw_table = generate_table(tw_data, 'ç¹ä½“ç”µè§†é¢‘é“åˆ—è¡¨')
          sw_table = generate_table(sw_data, 'ç®€ä½“ç”µè§†é¢‘é“åˆ—è¡¨')

          def generate_detailed(data, table, lang):
              return {
                  'table_format': table,
                  'channels': data,
                  'summary': {
                      'total_channels': len(data),
                      'total_programs': sum(ch['program_count'] for ch in data),
                      'channels_with_epg': len([ch for ch in data if ch['epg_status'] == 'â– ']),
                      'date_generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                  }
              }

          tw_detailed = generate_detailed(tw_data, tw_table, 'ç¹ä½“')
          sw_detailed = generate_detailed(sw_data, sw_table, 'ç®€ä½“')

          output_files = [
              ('traditional_channels_table.json', tw_table),
              ('simplified_channels_table.json', sw_table),
              ('traditional_channels_detailed.json', tw_detailed),
              ('simplified_channels_detailed.json', sw_detailed)
          ]

          for filename, content in output_files:
              with open(filename, 'w', encoding='utf-8') as f:
                  json.dump(content, f, ensure_ascii=False, indent=2)

          print(f"âœ… ç¹é«”é›»è¦–å°: å…±åˆ†æ {len(tw_data)} ä¸ªé¢‘é“")
          print(f"âœ… ç®€ä½“ç”µè§†å°: å…±åˆ†æ {len(sw_data)} ä¸ªé¢‘é“")
          print(f"âœ… ç¹é«”ç¯€ç›®ç¸½æ•¸: {sum(ch['program_count'] for ch in tw_data)}")
          print(f"âœ… ç®€ä½“ç¯€ç›®ç¸½æ•¸: {sum(ch['program_count'] for ch in sw_data)}")
          print("ğŸ“ å·²ç”Ÿæˆæ–‡ä»¶:")
          for filename, _ in output_files:
              print(f"   - {filename}")
          EOF

      - name: Extract channel names (ä¿æŒé¡ºåº)
        run: |
          python <<'EOF'
          import xml.etree.ElementTree as ET
          import re

          def extract_names(xml_file, output_file):
              try:
                  tree = ET.parse(xml_file)
                  root = tree.getroot()
                  ns = {}
                  if root.tag.startswith('{'):
                      ns_uri = root.tag.split('}')[0][1:]
                      ns = {'ns': ns_uri}
                  
                  seen = set()
                  channels = []
                  channel_xpath = 'ns:channel' if ns else 'channel'
                  for c in root.findall(channel_xpath, ns):
                      name = c.get('display-name')
                      if not name:
                          name_elem = c.find('ns:display-name' if ns else 'display-name', ns)
                          name = name_elem.text.strip() if name_elem is not None and name_elem.text else None
                      name = name.strip() if name else c.get('id', '')
                      if name and name not in seen:
                          seen.add(name)
                          channels.append(name)

                  with open(output_file, 'w', encoding='utf-8') as f:
                      f.write('\n'.join(channels))
                  print(f"âœ… {xml_file}: å…±æå– {len(channels)} ä¸ªé¢‘é“ï¼Œå·²ä¿å­˜åˆ° {output_file}")
              except Exception as e:
                  print(f"âŒ è§£æ {xml_file} å‡ºé”™: {str(e)}")

          extract_names('twepg.xml', 'ç¹é«”é›»è¦–å°ç›®éŒ„.txt')
          extract_names('swepg.xml', 'ç®€ä½“ç”µè§†å°ç›®å½•.txt')
          EOF

      - name: Generate README with statistics
        run: |
          python <<'EOF'
          import json
          from datetime import datetime

          # è¯»å–ç”Ÿæˆçš„JSONæ–‡ä»¶
          with open('traditional_channels_detailed.json', 'r', encoding='utf-8') as f:
              tw_data = json.load(f)
          
          with open('simplified_channels_detailed.json', 'r', encoding='utf-8') as f:
              sw_data = json.load(f)

          # è®¡ç®—EPGè¦†ç›–å¤©æ•°
          def calculate_epg_days(time_range_str):
              if "è‡³" not in time_range_str or "æš‚æ— èŠ‚ç›®æ•°æ®" in time_range_str:
                  return 0
              try:
                  parts = time_range_str.split(" è‡³ ")
                  if len(parts) == 2:
                      start_str = parts[0]
                      end_str = parts[1]
                      start_date = datetime.strptime(start_str, '%Y-%m-%d %H:%M:%S')
                      end_date = datetime.strptime(end_str, '%Y-%m-%d %H:%M:%S')
                      days = (end_date - start_date).days + 1
                      return max(1, days)  # è‡³å°‘1å¤©
              except:
                  pass
              return 0

          # è®¡ç®—å¹³å‡EPGå¤©æ•°
          tw_epg_days = [calculate_epg_days(ch['time_range']) for ch in tw_data['channels'] if calculate_epg_days(ch['time_range']) > 0]
          sw_epg_days = [calculate_epg_days(ch['time_range']) for ch in sw_data['channels'] if calculate_epg_days(ch['time_range']) > 0]

          avg_tw_days = sum(tw_epg_days) / len(tw_epg_days) if tw_epg_days else 0
          avg_sw_days = sum(sw_epg_days) / len(sw_epg_days) if sw_epg_days else 0

          # ç”ŸæˆREADMEå†…å®¹
          readme_content = f"""# EPG é¢‘é“ç»Ÿè®¡æŠ¥å‘Š

æœ€åæ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ

### ç¹ä½“ç”µè§†å°
- **æ€»é¢‘é“æ•°**: {tw_data['summary']['total_channels']}
- **æœ‰EPGçš„é¢‘é“**: {tw_data['summary']['channels_with_epg']}
- **èŠ‚ç›®æ€»æ•°**: {tw_data['summary']['total_programs']}
- **å¹³å‡EPGè¦†ç›–å¤©æ•°**: {avg_tw_days:.1f} å¤©

### ç®€ä½“ç”µè§†å°  
- **æ€»é¢‘é“æ•°**: {sw_data['summary']['total_channels']}
- **æœ‰EPGçš„é¢‘é“**: {sw_data['summary']['channels_with_epg']}
- **èŠ‚ç›®æ€»æ•°**: {sw_data['summary']['total_programs']}
- **å¹³å‡EPGè¦†ç›–å¤©æ•°**: {avg_sw_days:.1f} å¤©

## ğŸ“º é¢‘é“è¯¦æƒ…

### ç¹ä½“ç”µè§†é¢‘é“
| é¢‘é“ID | é¢‘é“åç§° | èŠ‚ç›®æ•°é‡ | æ—¶é—´èŒƒå›´ | EPGçŠ¶æ€ |
|--------|----------|----------|----------|---------|
"""

          # æ·»åŠ ç¹ä½“é¢‘é“è¡¨æ ¼
          for channel in tw_data['channels'][:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªé¢‘é“
              readme_content += f"| {channel['channel_id']} | {channel['channel_name']} | {channel['program_count']} | {channel['time_range']} | {channel['epg_status']} |\n"

          readme_content += f"\n... ä»¥åŠ {len(tw_data['channels']) - 20} ä¸ªæ›´å¤šé¢‘é“\n\n"

          readme_content += """### ç®€ä½“ç”µè§†é¢‘é“
| é¢‘é“ID | é¢‘é“åç§° | èŠ‚ç›®æ•°é‡ | æ—¶é—´èŒƒå›´ | EPGçŠ¶æ€ |
|--------|----------|----------|----------|---------|
"""

          # æ·»åŠ ç®€ä½“é¢‘é“è¡¨æ ¼
          for channel in sw_data['channels'][:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªé¢‘é“
              readme_content += f"| {channel['channel_id']} | {channel['channel_name']} | {channel['program_count']} | {channel['time_range']} | {channel['epg_status']} |\n"

          readme_content += f"\n... ä»¥åŠ {len(sw_data['channels']) - 20} ä¸ªæ›´å¤šé¢‘é“\n\n"

          readme_content += """## ğŸ“ æ–‡ä»¶è¯´æ˜

- `ç¹é«”é›»è¦–å°ç›®éŒ„.txt` - æ‰€æœ‰ç¹ä½“ç”µè§†å°åç§°åˆ—è¡¨
- `ç®€ä½“ç”µè§†å°ç›®å½•.txt` - æ‰€æœ‰ç®€ä½“ç”µè§†å°åç§°åˆ—è¡¨  
- `traditional_channels_detailed.json` - ç¹ä½“é¢‘é“è¯¦ç»†æ•°æ®
- `simplified_channels_detailed.json` - ç®€ä½“é¢‘é“è¯¦ç»†æ•°æ®

---

*æœ¬æŠ¥å‘Šç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆï¼Œæ¯6å°æ—¶æ›´æ–°ä¸€æ¬¡*"""

          # å†™å…¥README.md
          with open('README.md', 'w', encoding='utf-8') as f:
              f.write(readme_content)

          print("âœ… README.md å·²ç”Ÿæˆ")
          EOF

      - name: Commit and push results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # æäº¤ç”Ÿæˆçš„æ–‡ä»¶ï¼ŒåŒ…æ‹¬README.md
          git add "ç¹é«”é›»è¦–å°ç›®éŒ„.txt" "ç®€ä½“ç”µè§†å°ç›®å½•.txt" "*.json" "README.md"
          
          if git diff --cached --quiet; then
            echo "âš ï¸ æ²¡æœ‰éœ€è¦æäº¤çš„å˜æ›´"
          else
            git commit -m "Update channel lists, analysis and README (auto-generated)"
            for i in {1..3}; do
              if git push; then
                echo "âœ… æäº¤å¹¶æ¨é€æˆåŠŸ"
                exit 0
              else
                echo "âš ï¸ æ¨é€å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•ï¼ˆç¬¬ $i æ¬¡ï¼‰"
                git pull --rebase
              fi
            done
            echo "âŒ å¤šæ¬¡æ¨é€å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å¤„ç†"
            exit 1
          fi
