name: Channels Table Update

on:
  schedule:
    # æ¯å¤©å‡Œæ™¨3ç‚¹è¿è¡Œ (UTCæ—¶é—´)
    - cron: '0 3 * * *'
  workflow_dispatch:  # å…è®¸æ‰‹åŠ¨è§¦å‘
  push:
    paths:
      - '.github/workflows/channels-table.yml'
    branches: [ main ]

permissions:
  contents: write

jobs:
  generate-channels-table:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests pandas
        
    - name: Fetch and process JSON data
      env:
        JSON_URL: "https://raw.githubusercontent.com/zzq1234567890/epg/main/traditional_channels_table.json"
      run: |
        python << 'EOF'
import json
import requests
import pandas as pd
from datetime import datetime
import re

def fetch_json_data(url):
    """è·å–JSONæ•°æ®"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"Error fetching JSON data: {e}")
        return None

def process_channels_data(json_data):
    """å¤„ç†é¢‘é“æ•°æ®"""
    channels = []
    
    if not json_data:
        return channels
    
    try:
        # æ ¹æ®å®é™…çš„JSONç»“æ„è¿›è¡Œè°ƒæ•´
        # å‡è®¾JSONæ˜¯ä¸€ä¸ªæ•°ç»„æˆ–å¯¹è±¡
        if isinstance(json_data, list):
            # JSONæ˜¯æ•°ç»„æ ¼å¼
            for i, item in enumerate(json_data):
                if isinstance(item, dict):
                    channel = extract_channel_info(item, i)
                    if channel:
                        channels.append(channel)
        elif isinstance(json_data, dict):
            # JSONæ˜¯å¯¹è±¡æ ¼å¼ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªé¢‘é“
            for key, item in json_data.items():
                if isinstance(item, dict):
                    channel = extract_channel_info(item, key)
                    if channel:
                        channels.append(channel)
                elif isinstance(item, list):
                    for i, sub_item in enumerate(item):
                        if isinstance(sub_item, dict):
                            channel = extract_channel_info(sub_item, f"{key}_{i}")
                            if channel:
                                channels.append(channel)
        
        return channels
    except Exception as e:
        print(f"Error processing data: {e}")
        import traceback
        traceback.print_exc()
        return []

def extract_channel_info(item, identifier):
    """ä»JSONå¯¹è±¡ä¸­æå–é¢‘é“ä¿¡æ¯"""
    try:
        channel = {
            'id': identifier,
            'name': '',
            'group': '',
            'url': '',
            'logo': '',
            'epg_id': '',
            'options': '',
            'status': 'æœªçŸ¥'
        }
        
        # å°è¯•ä»ä¸åŒå¯èƒ½çš„å­—æ®µåä¸­è·å–æ•°æ®
        name_mapping = [
            'name', 'channel', 'title', 'channel_name', 
            'display-name', 'display_name', 'displayName'
        ]
        
        for field in name_mapping:
            if field in item and item[field]:
                channel['name'] = str(item[field]).strip()
                break
        
        # è·å–åˆ†ç»„ä¿¡æ¯
        group_mapping = ['group', 'group-title', 'group_title', 'category', 'type']
        for field in group_mapping:
            if field in item and item[field]:
                channel['group'] = str(item[field]).strip()
                break
        
        # è·å–URL
        url_mapping = ['url', 'link', 'stream_url', 'source', 'address']
        for field in url_mapping:
            if field in item and item[field]:
                channel['url'] = str(item[field]).strip()
                break
        
        # è·å–logo
        logo_mapping = ['logo', 'icon', 'image', 'thumbnail']
        for field in logo_mapping:
            if field in item and item[field]:
                channel['logo'] = str(item[field]).strip()
                break
        
        # è·å–EPG ID
        epg_mapping = ['epg_id', 'epg', 'tvg-id', 'tvg_id', 'id']
        for field in epg_mapping:
            if field in item and item[field]:
                channel['epg_id'] = str(item[field]).strip()
                break
        
        # è·å–å…¶ä»–é€‰é¡¹
        if 'options' in item:
            channel['options'] = item['options']
        
        # æ£€æŸ¥URLæœ‰æ•ˆæ€§
        if channel['url'] and channel['url'].startswith(('http://', 'https://', 'rtmp://', 'rtsp://')):
            channel['status'] = 'å¯ç”¨'
        else:
            channel['status'] = 'æ— æ•ˆé“¾æ¥'
        
        return channel
    except Exception as e:
        print(f"Error extracting channel info: {e}")
        return None

def generate_markdown_table(channels):
    """ç”ŸæˆMarkdownè¡¨æ ¼"""
    if not channels:
        return "## ğŸ“º ä¼ ç»Ÿç”µè§†é¢‘é“åˆ—è¡¨\n\næš‚æ— æ•°æ®"
    
    # æ’åºï¼šæŒ‰åˆ†ç»„å’Œåç§°æ’åº
    sorted_channels = sorted(channels, key=lambda x: (x['group'], x['name']))
    
    # åˆ†ç»„ç»Ÿè®¡
    df = pd.DataFrame(sorted_channels)
    groups = df['group'].unique() if 'group' in df.columns and len(df) > 0 else []
    
    markdown = f"""## ğŸ“º ä¼ ç»Ÿç”µè§†é¢‘é“åˆ—è¡¨

**æœ€åæ›´æ–°:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº:** [traditional_channels_table.json](https://github.com/zzq1234567890/epg/blob/main/traditional_channels_table.json)
**æ€»é¢‘é“æ•°:** {len(channels)} ä¸ª

### ğŸ“Š åˆ†ç»„ç»Ÿè®¡
"""
    
    # æ·»åŠ åˆ†ç»„ç»Ÿè®¡
    if len(groups) > 0:
        group_stats = df['group'].value_counts().reset_index()
        group_stats.columns = ['åˆ†ç»„', 'é¢‘é“æ•°é‡']
        
        markdown += "| åˆ†ç»„ | é¢‘é“æ•°é‡ |\n|------|----------|\n"
        for _, row in group_stats.iterrows():
            group_name = str(row['åˆ†ç»„']) if pd.notna(row['åˆ†ç»„']) else "æœªåˆ†ç»„"
            markdown += f"| {group_name} | {row['é¢‘é“æ•°é‡']} |\n"
    else:
        markdown += "*æš‚æ— åˆ†ç»„ä¿¡æ¯*\n"
    
    # è¯¦ç»†çš„é¢‘é“è¡¨æ ¼
    markdown += f"""

### ğŸ“‹ é¢‘é“è¯¦æƒ… (å…± {len(channels)} ä¸ªé¢‘é“)

| åºå· | é¢‘é“åç§° | åˆ†ç»„ | EPG ID | çŠ¶æ€ | Logo |
|------|----------|------|--------|------|------|
"""
    
    for i, channel in enumerate(sorted_channels, 1):
        # å¤„ç†é¢‘é“åç§°æ˜¾ç¤º
        name = channel['name']
        if len(name) > 20:
            name = name[:17] + "..."
        
        # å¤„ç†åˆ†ç»„æ˜¾ç¤º
        group = channel['group']
        if len(group) > 15:
            group = group[:12] + "..."
        if not group:
            group = "é»˜è®¤"
        
        # å¤„ç†EPG IDæ˜¾ç¤º
        epg_id = channel['epg_id']
        if len(epg_id) > 12:
            epg_id = epg_id[:9] + "..."
        
        # å¤„ç†Logoæ˜¾ç¤º
        logo = ""
        if channel['logo'] and channel['logo'].startswith('http'):
            logo = f"![Logo]({channel['logo']})"
        elif channel['logo']:
            logo = channel['logo'][:15] + "..." if len(channel['logo']) > 15 else channel['logo']
        
        # çŠ¶æ€é¢œè‰²
        status = channel['status']
        status_badge = ""
        if status == 'å¯ç”¨':
            status_badge = "ğŸŸ¢ å¯ç”¨"
        elif status == 'æ— æ•ˆé“¾æ¥':
            status_badge = "ğŸ”´ æ— æ•ˆ"
        else:
            status_badge = "ğŸŸ¡ æœªçŸ¥"
        
        markdown += f"| {i} | {name} | {group} | `{epg_id}` | {status_badge} | {logo} |\n"
    
    # æ·»åŠ ç»Ÿè®¡å’Œè¯´æ˜
    markdown += f"""

### ğŸ“ˆ æ•°æ®ç»Ÿè®¡
- **æ€»é¢‘é“æ•°:** {len(channels)} ä¸ª
- **å¯ç”¨çš„é¢‘é“:** {len([c for c in channels if c['status'] == 'å¯ç”¨'])} ä¸ª
- **åˆ†ç»„æ•°é‡:** {len(groups)} ä¸ª
- **æœ€åæ›´æ–°:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### ğŸ”§ ä½¿ç”¨è¯´æ˜
1. æ•°æ®æ¯æ—¥è‡ªåŠ¨æ›´æ–°
2. æŒ‰åˆ†ç»„å’Œåç§°æ’åº
3. ğŸŸ¢ è¡¨ç¤ºé“¾æ¥å¯ç”¨
4. ğŸ”´ è¡¨ç¤ºé“¾æ¥æ— æ•ˆ
5. ç‚¹å‡»é¢‘é“åç§°å¯æŸ¥çœ‹è¯¦ç»†é…ç½®

---
*æ•°æ®æ¥æº: [zzq1234567890/epg](https://github.com/zzq1234567890/epg)*
"""
    
    return markdown

def update_readme(markdown_content, section_title="ğŸ“º ä¼ ç»Ÿç”µè§†é¢‘é“åˆ—è¡¨"):
    """æ›´æ–°README.mdæ–‡ä»¶"""
    try:
        # è¯»å–ç°æœ‰README
        with open('README.md', 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾é¢‘é“åˆ—è¡¨éƒ¨åˆ†
        start_marker = f"## {section_title}"
        start_index = content.find(start_marker)
        
        if start_index != -1:
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªä¸»è¦ç« èŠ‚
            next_section_match = re.search(r'\n## [^#]', content[start_index:])
            
            if next_section_match:
                end_index = start_index + next_section_match.start()
                # æ›¿æ¢æŒ‡å®šéƒ¨åˆ†
                new_content = content[:start_index] + markdown_content + content[end_index:]
            else:
                # å¦‚æœæ˜¯æœ€åä¸€éƒ¨åˆ†
                new_content = content[:start_index] + markdown_content
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾
            # å…ˆæŸ¥æ‰¾åˆé€‚çš„æ’å…¥ä½ç½®ï¼ˆåœ¨æœ€åä¸€ä¸ª##æ ‡é¢˜åï¼‰
            last_section_match = re.search(r'\n## [^#].*(?=\n*$)', content, re.DOTALL)
            
            if last_section_match:
                insert_pos = last_section_match.end()
                new_content = content[:insert_pos] + '\n\n' + markdown_content + content[insert_pos:]
            else:
                # å¦‚æœæ²¡æœ‰ä»»ä½•##æ ‡é¢˜ï¼Œç›´æ¥æ·»åŠ åˆ°æœ«å°¾
                new_content = content.rstrip() + '\n\n' + markdown_content
        
        # å†™å…¥æ–‡ä»¶
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"æˆåŠŸæ›´æ–° README.md çš„ '{section_title}' éƒ¨åˆ†")
        
    except FileNotFoundError:
        # å¦‚æœREADMEä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(f"# é¢‘é“åˆ—è¡¨\n\n{markdown_content}")
        print("åˆ›å»ºäº†æ–°çš„ README.md æ–‡ä»¶")
    except Exception as e:
        print(f"æ›´æ–° README.md æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def main():
    print("å¼€å§‹è·å–JSONæ•°æ®...")
    json_data = fetch_json_data('${{ env.JSON_URL }}')
    
    if json_data is None:
        print("è·å–æ•°æ®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®")
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é»˜è®¤æ•°æ®æˆ–é€€å‡º
        return
    
    print("å¤„ç†é¢‘é“æ•°æ®...")
    channels = process_channels_data(json_data)
    
    print(f"æ‰¾åˆ° {len(channels)} ä¸ªé¢‘é“")
    
    print("ç”ŸæˆMarkdownè¡¨æ ¼...")
    markdown_table = generate_markdown_table(channels)
    
    print("æ›´æ–°README.md...")
    update_readme(markdown_table)
    
    print("å®Œæˆï¼")

if __name__ == "__main__":
    main()
EOF

    - name: Commit and Push changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add README.md
        git diff --quiet && git diff --staged --quiet || (git commit -m "ğŸ“º æ›´æ–°é¢‘é“åˆ—è¡¨è¡¨æ ¼ $(date +'%Y-%m-%d %H:%M')" && git push)
        
    - name: Create Summary
      run: |
        echo "## é¢‘é“åˆ—è¡¨æ›´æ–°å®Œæˆ" >> $GITHUB_STEP_SUMMARY
        echo "âœ… JSONæ•°æ®å·²æˆåŠŸå¤„ç†å¹¶æ›´æ–°åˆ°README.md" >> $GITHUB_STEP_SUMMARY
        echo "ğŸ“Š è¡¨æ ¼å·²è‡ªåŠ¨ç”Ÿæˆå¹¶æ’å…¥" >> $GITHUB_STEP_SUMMARY
        echo "â° ä¸‹ä¸€è½®è‡ªåŠ¨æ›´æ–°ï¼šæ˜å¤© 03:00 UTC" >> $GITHUB_STEP_SUMMARY