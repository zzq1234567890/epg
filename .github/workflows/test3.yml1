name: æå–é›»è¦–å°åç¨±å’Œç”Ÿæˆåˆ†æå ±å‘Š

on:
  schedule:
    - cron: '45 */1 * * *'   # æ¯6å°æ—¶è¿è¡Œä¸€æ¬¡
  workflow_dispatch:          # æ”¯æŒæ‰‹åŠ¨è§¦å‘

jobs:
  extract:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout current repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Download XML files
        run: |
          curl -L -o twepg.xml https://raw.githubusercontent.com/zzq1234567890/epg/main/twepg.xml
          curl -L -o swepg.xml https://raw.githubusercontent.com/zzq1234567890/epg/main/swepg.xml

      - name: Extract channel names and generate JSON analysis
        run: |
          python <<'EOF'
          import xml.etree.ElementTree as ET
          import json
          from datetime import datetime

          def analyze_epg_data(xml_file):
              try:
                  tree = ET.parse(xml_file)
                  root = tree.getroot()
                  
                  # å­˜å‚¨é¢‘é“ä¿¡æ¯
                  channels_info = {}
                  
                  # ç¬¬ä¸€éï¼šæå–æ‰€æœ‰é¢‘é“åç§°
                  for channel in root.findall('channel'):
                      channel_id = channel.get('id')
                      name = channel.get('display-name') or channel.findtext('display-name') or channel_id
                      
                      if name and name.strip():
                          channels_info[channel_id] = {
                              'name': name.strip(),
                              'program_count': 0,
                              'start_times': [],
                              'end_times': []
                          }
                  
                  # ç¬¬äºŒéï¼šç»Ÿè®¡èŠ‚ç›®ä¿¡æ¯
                  for programme in root.findall('programme'):
                      channel_id = programme.get('channel')
                      start_time = programme.get('start')
                      end_time = programme.get('stop')
                      
                      if channel_id in channels_info:
                          channels_info[channel_id]['program_count'] += 1
                          
                          if start_time:
                              channels_info[channel_id]['start_times'].append(start_time)
                          if end_time:
                              channels_info[channel_id]['end_times'].append(end_time)
                  
                  # å¤„ç†æ¯ä¸ªé¢‘é“çš„æ—¶é—´èŒƒå›´
                  result = []
                  for channel_id, info in channels_info.items():
                      # æ ¼å¼åŒ–æ—¶é—´èŒƒå›´
                      time_range = "æš‚æ— èŠ‚ç›®æ•°æ®"
                      if info['start_times'] and info['end_times']:
                          # æ‰¾åˆ°æœ€æ—©å¼€å§‹æ—¶é—´å’Œæœ€æ™šç»“æŸæ—¶é—´
                          sorted_starts = sorted(info['start_times'])
                          sorted_ends = sorted(info['end_times'])
                          
                          earliest_start = sorted_starts[0]
                          latest_end = sorted_ends[-1]
                          
                          # è½¬æ¢æ—¶é—´æ ¼å¼ï¼šä» YYYYMMDDHHMMSS åˆ° YYYY-MM-DD HH:MM:SS
                          try:
                              start_dt = datetime.strptime(earliest_start, '%Y%m%d%H%M%S')
                              end_dt = datetime.strptime(latest_end, '%Y%m%d%H%M%S')
                              
                              start_str = start_dt.strftime('%Y-%m-%d %H:%M:%S')
                              end_str = end_dt.strftime('%Y-%m-%d %H:%M:%S')
                              time_range = f"{start_str} è‡³ {end_str}"
                          except ValueError:
                              time_range = f"{earliest_start} è‡³ {latest_end}"
                      
                      # EPGçŠ¶æ€ï¼ˆæ ¹æ®èŠ‚ç›®æ•°é‡åˆ¤æ–­ï¼‰
                      epg_status = "â– " if info['program_count'] > 0 else "â–¡"
                      
                      result.append({
                          'channel_id': channel_id,
                          'channel_name': info['name'],
                          'program_count': info['program_count'],
                          'time_range': time_range,
                          'epg_status': epg_status
                      })
                  
                  # æŒ‰é¢‘é“åç§°æ’åº
                  result.sort(key=lambda x: x['channel_name'])
                  
                  return result
                  
              except Exception as e:
                  print(f"âŒ è§£æ {xml_file} å‡ºé”™: {e}")
                  return []

          # åˆ†æä¸¤ä¸ª XML æ–‡ä»¶
          print("å¼€å§‹åˆ†æ EPG æ•°æ®...")
          tw_data = analyze_epg_data('twepg.xml')
          sw_data = analyze_epg_data('swepg.xml')

          # ç”Ÿæˆç¹ä½“é¢‘é“ JSONï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰
          tw_table_data = {
              'title': 'ç¹ä½“ç”µè§†é¢‘é“åˆ—è¡¨',
              'headers': ['é¢‘é“Id', 'é¢‘é“åç§°', 'èŠ‚ç›®æ•°é‡', 'æ—¶é—´èŒƒå›´', 'EPGçŠ¶æ€'],
              'data': []
          }

          for channel in tw_data:
              tw_table_data['data'].append([
                  channel['channel_id'],
                  channel['channel_name'],
                  channel['program_count'],
                  channel['time_range'],
                  channel['epg_status']
              ])

          # ç”Ÿæˆç®€ä½“é¢‘é“ JSONï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰
          sw_table_data = {
              'title': 'ç®€ä½“ç”µè§†é¢‘é“åˆ—è¡¨',
              'headers': ['é¢‘é“Id', 'é¢‘é“åç§°', 'èŠ‚ç›®æ•°é‡', 'æ—¶é—´èŒƒå›´', 'EPGçŠ¶æ€'],
              'data': []
          }

          for channel in sw_data:
              sw_table_data['data'].append([
                  channel['channel_id'],
                  channel['channel_name'],
                  channel['program_count'],
                  channel['time_range'],
                  channel['epg_status']
              ])

          # ä¿å­˜ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ JSON æ–‡ä»¶
          with open('traditional_channels_table.json', 'w', encoding='utf-8') as f:
              json.dump(tw_table_data, f, ensure_ascii=False, indent=2)

          with open('simplified_channels_table.json', 'w', encoding='utf-8') as f:
              json.dump(sw_table_data, f, ensure_ascii=False, indent=2)

          # åŒæ—¶ç”Ÿæˆè¯¦ç»†ç‰ˆæœ¬ï¼ˆåŒ…å«åŸå§‹æ•°æ®ï¼‰
          tw_detailed = {
              'table_format': tw_table_data,
              'channels': tw_data,
              'summary': {
                  'total_channels': len(tw_data),
                  'total_programs': sum(ch['program_count'] for ch in tw_data),
                  'channels_with_epg': len([ch for ch in tw_data if ch['epg_status'] == 'â– ']),
                  'date_generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
              }
          }

          sw_detailed = {
              'table_format': sw_table_data,
              'channels': sw_data,
              'summary': {
                  'total_channels': len(sw_data),
                  'total_programs': sum(ch['program_count'] for ch in sw_data),
                  'channels_with_epg': len([ch for ch in sw_data if ch['epg_status'] == 'â– ']),
                  'date_generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
              }
          }

          with open('traditional_channels_detailed.json', 'w', encoding='utf-8') as f:
              json.dump(tw_detailed, f, ensure_ascii=False, indent=2)

          with open('simplified_channels_detailed.json', 'w', encoding='utf-8') as f:
              json.dump(sw_detailed, f, ensure_ascii=False, indent=2)

          print(f"âœ… ç¹é«”é›»è¦–å°: å…±åˆ†æ {len(tw_data)} ä¸ªé¢‘é“")
          print(f"âœ… ç®€ä½“ç”µè§†å°: å…±åˆ†æ {len(sw_data)} ä¸ªé¢‘é“")
          print(f"âœ… ç¹é«”ç¯€ç›®ç¸½æ•¸: {sum(ch['program_count'] for ch in tw_data)}")
          print(f"âœ… ç®€ä½“ç¯€ç›®ç¸½æ•¸: {sum(ch['program_count'] for ch in sw_data)}")
          print("ğŸ“ å·²ç”Ÿæˆæ–‡ä»¶:")
          print("   - traditional_channels_table.json (ç¹ä½“é¢‘é“è¡¨æ ¼æ ¼å¼)")
          print("   - simplified_channels_table.json (ç®€ä½“é¢‘é“è¡¨æ ¼æ ¼å¼)")
          print("   - traditional_channels_detailed.json (ç¹ä½“é¢‘é“è¯¦ç»†æ•°æ®)")
          print("   - simplified_channels_detailed.json (ç®€ä½“é¢‘é“è¯¦ç»†æ•°æ®)")
          EOF

      - name: Generate README.md with tables
        run: |
          python <<'EOF'
          import json
          from datetime import datetime

          # è¯»å– JSON æ•°æ®
          with open('traditional_channels_table.json', 'r', encoding='utf-8') as f:
              tw_data = json.load(f)
          
          with open('simplified_channels_table.json', 'r', encoding='utf-8') as f:
              sw_data = json.load(f)

          with open('traditional_channels_detailed.json', 'r', encoding='utf-8') as f:
              tw_detailed = json.load(f)

          with open('simplified_channels_detailed.json', 'r', encoding='utf-8') as f:
              sw_detailed = json.load(f)

          # ç”Ÿæˆ Markdown è¡¨æ ¼å‡½æ•°
          def generate_markdown_table(data, title, max_rows=20):
              if not data['data']:
                  return f"## {title}\\n\\næš‚æ— æ•°æ®\\n"
              
              headers = data['headers']
              table_rows = data['data'][:max_rows]
              
              # è¡¨å¤´
              md = f"## {title}\\n\\n"
              md += "| " + " | ".join(headers) + " |\\n"
              md += "|" + " | ".join(["---"] * len(headers)) + "|\\n"
              
              # è¡¨æ ¼å†…å®¹
              for row in table_rows:
                  md += "| " + " | ".join(str(cell) for cell in row) + " |\\n"
              
              # å¦‚æœæ•°æ®è¢«æˆªæ–­ï¼Œæ˜¾ç¤ºæç¤º
              if len(data['data']) > max_rows:
                  md += f"\\n*ä»…æ˜¾ç¤ºå‰ {max_rows} ä¸ªé¢‘é“ï¼Œå…± {len(data['data'])} ä¸ªé¢‘é“*\\n"
              
              return md

          # ç”Ÿæˆ README å†…å®¹
          readme_content = f"""# ğŸ“º EPG é¢‘é“åˆ—è¡¨

æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ç»Ÿè®¡æ¦‚è§ˆ

### ç¹ä½“é¢‘é“
- **æ€»é¢‘é“æ•°**: {tw_detailed['summary']['total_channels']}
- **æ€»èŠ‚ç›®æ•°**: {tw_detailed['summary']['total_programs']}
- **æœ‰EPGé¢‘é“**: {tw_detailed['summary']['channels_with_epg']}
- **æ›´æ–°æ—¶é—´**: {tw_detailed['summary']['date_generated']}

### ç®€ä½“é¢‘é“
- **æ€»é¢‘é“æ•°**: {sw_detailed['summary']['total_channels']}
- **æ€»èŠ‚ç›®æ•°**: {sw_detailed['summary']['total_programs']}
- **æœ‰EPGé¢‘é“**: {sw_detailed['summary']['channels_with_epg']}
- **æ›´æ–°æ—¶é—´**: {sw_detailed['summary']['date_generated']}

---

{generate_markdown_table(tw_data, 'ç¹ä½“ç”µè§†é¢‘é“åˆ—è¡¨')}

---

{generate_markdown_table(sw_data, 'ç®€ä½“ç”µè§†é¢‘é“åˆ—è¡¨')}

## è¯´æ˜

- **é¢‘é“Id**: é¢‘é“çš„å”¯ä¸€æ ‡è¯†ç¬¦
- **é¢‘é“åç§°**: é¢‘é“çš„æ˜¾ç¤ºåç§°
- **èŠ‚ç›®æ•°é‡**: è¯¥é¢‘é“åŒ…å«çš„èŠ‚ç›®æ€»æ•°
- **æ—¶é—´èŒƒå›´**: èŠ‚ç›®æ—¶é—´è¦†ç›–èŒƒå›´
- **EPGçŠ¶æ€**: â–  è¡¨ç¤ºæœ‰èŠ‚ç›®æ•°æ®ï¼Œâ–¡ è¡¨ç¤ºæ— èŠ‚ç›®æ•°æ®

> æ•°æ®æ¯6å°æ—¶è‡ªåŠ¨æ›´æ–°ä¸€æ¬¡
"""

          # å†™å…¥ README.md
          with open('README.md', 'w', encoding='utf-8') as f:
              f.write(readme_content)

          print("âœ… README.md å·²ç”Ÿæˆ")
          EOF

      - name: Extract channel names (ä¿æŒé¡ºåº)
        run: |
          python <<'EOF'
          import xml.etree.ElementTree as ET

          def extract_names(xml_file, output_file):
              try:
                  tree = ET.parse(xml_file)
                  root = tree.getroot()
                  seen = set()
                  channels = []
                  for c in root.findall('channel'):
                      name = c.get('display-name') or c.findtext('display-name') or c.get('id', '')
                      if name and name.strip() and name not in seen:
                          seen.add(name)
                          channels.append(name.strip())

                  with open(output_file, 'w', encoding='utf-8') as f:
                      f.write('\n'.join(channels))
                  print(f"âœ… {xml_file}: å…±æå– {len(channels)} ä¸ªé¢‘é“ï¼Œå·²ä¿å­˜åˆ° {output_file}")
              except Exception as e:
                  print(f"âŒ è§£æ {xml_file} å‡ºé”™: {e}")

          extract_names('twepg.xml', 'ç¹é«”é›»è¦–å°ç›®éŒ„.txt')
          extract_names('swepg.xml', 'ç®€ä½“ç”µè§†å°ç›®å½•.txt')
          EOF

      - name: Commit and push results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add "ç¹é«”é›»è¦–å°ç›®éŒ„.txt" "ç®€ä½“ç”µè§†å°ç›®å½•.txt" "traditional_channels_table.json" "simplified_channels_table.json" "traditional_channels_detailed.json" "simplified_channels_detailed.json" "README.md"
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update channel lists, JSON analysis and README.md"
            git push
          fi                  
                  # æŒ‰é¢‘é“åç§°æ’åº
                  result.sort(key=lambda x: x['channel_name'])
                  
                  return result
                  
              except Exception as e:
                  print(f"âŒ è§£æ {xml_file} å‡ºé”™: {e}")
                  return []

          # åˆ†æä¸¤ä¸ª XML æ–‡ä»¶
          print("å¼€å§‹åˆ†æ EPG æ•°æ®...")
          tw_data = analyze_epg_data('twepg.xml')
          sw_data = analyze_epg_data('swepg.xml')

          # ç”Ÿæˆç¹ä½“é¢‘é“ JSONï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰
          tw_table_data = {
              'title': 'ç¹ä½“ç”µè§†é¢‘é“åˆ—è¡¨',
              'headers': ['é¢‘é“Id', 'é¢‘é“åç§°', 'èŠ‚ç›®æ•°é‡', 'æ—¶é—´èŒƒå›´', 'EPGçŠ¶æ€'],
              'data': []
          }

          for channel in tw_data:
              tw_table_data['data'].append([
                  channel['channel_id'],
                  channel['channel_name'],
                  channel['program_count'],
                  channel['time_range'],
                  channel['epg_status']
              ])

          # ç”Ÿæˆç®€ä½“é¢‘é“ JSONï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰
          sw_table_data = {
              'title': 'ç®€ä½“ç”µè§†é¢‘é“åˆ—è¡¨',
              'headers': ['é¢‘é“Id', 'é¢‘é“åç§°', 'èŠ‚ç›®æ•°é‡', 'æ—¶é—´èŒƒå›´', 'EPGçŠ¶æ€'],
              'data': []
          }

          for channel in sw_data:
              sw_table_data['data'].append([
                  channel['channel_id'],
                  channel['channel_name'],
                  channel['program_count'],
                  channel['time_range'],
                  channel['epg_status']
              ])

          # ä¿å­˜ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ JSON æ–‡ä»¶
          with open('traditional_channels_table.json', 'w', encoding='utf-8') as f:
              json.dump(tw_table_data, f, ensure_ascii=False, indent=2)

          with open('simplified_channels_table.json', 'w', encoding='utf-8') as f:
              json.dump(sw_table_data, f, ensure_ascii=False, indent=2)

          # åŒæ—¶ç”Ÿæˆè¯¦ç»†ç‰ˆæœ¬ï¼ˆåŒ…å«åŸå§‹æ•°æ®ï¼‰
          tw_detailed = {
              'table_format': tw_table_data,
              'channels': tw_data,
              'summary': {
                  'total_channels': len(tw_data),
                  'total_programs': sum(ch['program_count'] for ch in tw_data),
                  'channels_with_epg': len([ch for ch in tw_data if ch['epg_status'] == 'â– ']),
                  'date_generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
              }
          }

          sw_detailed = {
              'table_format': sw_table_data,
              'channels': sw_data,
              'summary': {
                  'total_channels': len(sw_data),
                  'total_programs': sum(ch['program_count'] for ch in sw_data),
                  'channels_with_epg': len([ch for ch in sw_data if ch['epg_status'] == 'â– ']),
                  'date_generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
              }
          }

          with open('traditional_channels_detailed.json', 'w', encoding='utf-8') as f:
              json.dump(tw_detailed, f, ensure_ascii=False, indent=2)

          with open('simplified_channels_detailed.json', 'w', encoding='utf-8') as f:
              json.dump(sw_detailed, f, ensure_ascii=False, indent=2)

          print(f"âœ… ç¹é«”é›»è¦–å°: å…±åˆ†æ {len(tw_data)} ä¸ªé¢‘é“")
          print(f"âœ… ç®€ä½“ç”µè§†å°: å…±åˆ†æ {len(sw_data)} ä¸ªé¢‘é“")
          print(f"âœ… ç¹é«”ç¯€ç›®ç¸½æ•¸: {sum(ch['program_count'] for ch in tw_data)}")
          print(f"âœ… ç®€ä½“ç¯€ç›®ç¸½æ•¸: {sum(ch['program_count'] for ch in sw_data)}")
          print("ğŸ“ å·²ç”Ÿæˆæ–‡ä»¶:")
          print("   - traditional_channels_table.json (ç¹ä½“é¢‘é“è¡¨æ ¼æ ¼å¼)")
          print("   - simplified_channels_table.json (ç®€ä½“é¢‘é“è¡¨æ ¼æ ¼å¼)")
          print("   - traditional_channels_detailed.json (ç¹ä½“é¢‘é“è¯¦ç»†æ•°æ®)")
          print("   - simplified_channels_detailed.json (ç®€ä½“é¢‘é“è¯¦ç»†æ•°æ®)")
          EOF

      - name: Generate README.md with tables
        run: |
          python <<'EOF'
          import json
          from datetime import datetime

          # è¯»å– JSON æ•°æ®
          with open('traditional_channels_table.json', 'r', encoding='utf-8') as f:
              tw_data = json.load(f)
          
          with open('simplified_channels_table.json', 'r', encoding='utf-8') as f:
              sw_data = json.load(f)

          with open('traditional_channels_detailed.json', 'r', encoding='utf-8') as f:
              tw_detailed = json.load(f)

          with open('simplified_channels_detailed.json', 'r', encoding='utf-8') as f:
              sw_detailed = json.load(f)

          # ç”Ÿæˆ Markdown è¡¨æ ¼å‡½æ•°
          def generate_markdown_table(data, title, max_rows=20):
              if not data['data']:
                  return f"## {title}\\n\\næš‚æ— æ•°æ®\\n"
              
              headers = data['headers']
              table_rows = data['data'][:max_rows]  # é™åˆ¶æ˜¾ç¤ºè¡Œæ•°
              
              # è¡¨å¤´
              md = f"## {title}\\n\\n"
              md += "| " + " | ".join(headers) + " |\\n"
              md += "|" + " | ".join(["---"] * len(headers)) + "|\\n"
              
              # è¡¨æ ¼å†…å®¹
              for row in table_rows:
                  md += "| " + " | ".join(str(cell) for cell in row) + " |\\n"
              
              # å¦‚æœæ•°æ®è¢«æˆªæ–­ï¼Œæ˜¾ç¤ºæç¤º
              if len(data['data']) > max_rows:
                  md += f"\\n*ä»…æ˜¾ç¤ºå‰ {max_rows} ä¸ªé¢‘é“ï¼Œå…± {len(data['data'])} ä¸ªé¢‘é“*\\n"
              
              return md

          # ç”Ÿæˆ README å†…å®¹
          readme_content = f"""# ğŸ“º EPG é¢‘é“åˆ—è¡¨

æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ç»Ÿè®¡æ¦‚è§ˆ

### ç¹ä½“é¢‘é“
- **æ€»é¢‘é“æ•°**: {tw_detailed['summary']['total_channels']}
- **æ€»èŠ‚ç›®æ•°**: {tw_detailed['summary']['total_programs']}
- **æœ‰EPGé¢‘é“**: {tw_detailed['summary']['channels_with_epg']}
- **æ›´æ–°æ—¶é—´**: {tw_detailed['summary']['date_generated']}

### ç®€ä½“é¢‘é“
- **æ€»é¢‘é“æ•°**: {sw_detailed['summary']['total_channels']}
- **æ€»èŠ‚ç›®æ•°**: {sw_detailed['summary']['total_programs']}
- **æœ‰EPGé¢‘é“**: {sw_detailed['summary']['channels_with_epg']}
- **æ›´æ–°æ—¶é—´**: {sw_detailed['summary']['date_generated']}

---

{generate_markdown_table(tw_data, 'ç¹ä½“ç”µè§†é¢‘é“åˆ—è¡¨')}

---

{generate_markdown_table(sw_data, 'ç®€ä½“ç”µè§†é¢‘é“åˆ—è¡¨')}

## è¯´æ˜

- **é¢‘é“Id**: é¢‘é“çš„å”¯ä¸€æ ‡è¯†ç¬¦
- **é¢‘é“åç§°**: é¢‘é“çš„æ˜¾ç¤ºåç§°
- **èŠ‚ç›®æ•°é‡**: è¯¥é¢‘é“åŒ…å«çš„èŠ‚ç›®æ€»æ•°
- **æ—¶é—´èŒƒå›´**: èŠ‚ç›®æ—¶é—´è¦†ç›–èŒƒå›´
- **EPGçŠ¶æ€**: â–  è¡¨ç¤ºæœ‰èŠ‚ç›®æ•°æ®ï¼Œâ–¡ è¡¨ç¤ºæ— èŠ‚ç›®æ•°æ®

> æ•°æ®æ¯6å°æ—¶è‡ªåŠ¨æ›´æ–°ä¸€æ¬¡
"""

          # å†™å…¥ README.md
          with open('README.md', 'w', encoding='utf-8') as f:
              f.write(readme_content)

          print("âœ… README.md å·²ç”Ÿæˆ")
          EOF

      - name: Extract channel names (ä¿æŒé¡ºåº)
        run: |
          python <<'EOF'
          import xml.etree.ElementTree as ET

          def extract_names(xml_file, output_file):
              try:
                  tree = ET.parse(xml_file)
                  root = tree.getroot()
                  seen = set()
                  channels = []
                  for c in root.findall('channel'):
                      name = c.get('display-name') or c.findtext('display-name') or c.get('id', '')
                      if name and name.strip() and name not in seen:
                          seen.add(name)
                          channels.append(name.strip())

                  with open(output_file, 'w', encoding='utf-8') as f:
                      f.write('\n'.join(channels))
                  print(f"âœ… {xml_file}: å…±æå– {len(channels)} ä¸ªé¢‘é“ï¼Œå·²ä¿å­˜åˆ° {output_file}")
              except Exception as e:
                  print(f"âŒ è§£æ {xml_file} å‡ºé”™: {e}")

          extract_names('twepg.xml', 'ç¹é«”é›»è¦–å°ç›®éŒ„.txt')
          extract_names('swepg.xml', 'ç®€ä½“ç”µè§†å°ç›®å½•.txt')
          EOF

      - name: Commit and push results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add "ç¹é«”é›»è¦–å°ç›®éŒ„.txt" "ç®€ä½“ç”µè§†å°ç›®å½•.txt" "traditional_channels_table.json" "simplified_channels_table.json" "traditional_channels_detailed.json" "simplified_channels_detailed.json" "README.md"
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update channel lists, JSON analysis and README.md"
            git push
          fi
